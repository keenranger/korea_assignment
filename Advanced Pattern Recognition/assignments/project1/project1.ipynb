{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from sklearn.svm import SVC\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from array import array\n",
    "from os.path  import join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make custom dataloader to load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "input_path = './dataset'\n",
    "training_images_filepath = join(input_path, 'train-images.idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels.idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images.idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels.idx1-ubyte')\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "print(np.shape(x_train), np.shape(y_train), np.shape(x_test), np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "random_idx=np.random.randint(100)\n",
    "plt.imshow(x_train[random_idx], cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Describe steps for PCA\n",
    "1. First, scale the given data to same range (normalize data, mean  =0, variance = 1) You should fit the test data using train scale!\n",
    "2. Calculate covariance matrix, our dataset(28x28) should be converted to 1x28^2 array for calculation\n",
    "3. Using eigenvalue and eigenvector, get eigenvectors that having largest eigenvalue(maybe top n)\n",
    "4. Project normalized x to eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Write a pseudo-code for 2\n",
    "standardScaler = StandardScaler.fit(x_train) \n",
    "norm_x_train = standardScaler.transform(x_train)  \n",
    "Caluculate covaraince matrix cov by calculating np.matmul(norm_x_train.T, norm_x_train)  \n",
    "Find eigenvalue and eigenvector, and choose how many vector to use  \n",
    "(I personally prefer <5% data loss, which can be calculated by eig_w[0] / sum(eig_w))\n",
    "proj_x_train = np.matmul(eig_vectors, norm_x_train.T)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using the results from problem 1 to 3, construct a code that distinguishes 10 classes from 0 to 9 from a given dataset (MNIST) using PCA, so-called hand-writing recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale given data(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.1974863349995617e-18 0.9145408163265558 (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "standardScaler = StandardScaler().fit(x_train.reshape(-1,28*28))\n",
    "norm_x_train = standardScaler.transform(x_train.reshape(-1,28*28))\n",
    "print(np.mean(norm_x_train), np.var(norm_x_train), np.shape(norm_x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 784)\n"
     ]
    }
   ],
   "source": [
    "cov_mat = np.matmul(norm_x_train.T, norm_x_train)\n",
    "print(np.shape(cov_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find how many need for dataloss = 5%, It seems use top 10(774~) is reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx : 782, data_loss = 0.41936006580180335\n",
      "idx : 781, data_loss = 0.2777241421411718\n",
      "idx : 780, data_loss = 0.17646519086822218\n",
      "idx : 779, data_loss = 0.13359986367311344\n",
      "idx : 778, data_loss = 0.10416730405904916\n",
      "idx : 777, data_loss = 0.08366656754082634\n",
      "idx : 776, data_loss = 0.0705829537498866\n",
      "idx : 775, data_loss = 0.05843718975664753\n",
      "idx : 774, data_loss = 0.050656962405056676\n",
      "idx : 773, data_loss = 0.04624696887980538\n"
     ]
    }
   ],
   "source": [
    "idx = 782\n",
    "data_loss = 1\n",
    "while(data_loss > 0.05):\n",
    "    eig_w, eig_v = eigh(cov_mat, eigvals=(idx, 783))\n",
    "    data_loss = eig_w[0] / np.sum(eig_w)\n",
    "    print(\"idx : \" + str(idx) + \", data_loss = \" + str(data_loss))\n",
    "    idx -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.050656962405056676 (784, 10)\n"
     ]
    }
   ],
   "source": [
    "eig_w, eig_v = eigh(cov_mat, eigvals=(774, 783))\n",
    "data_loss = eig_w[0] / np.sum(eig_w)\n",
    "print(data_loss, np.shape(eig_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project train data to eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "proj_x_train = np.matmul(norm_x_train, eig_v)\n",
    "print(np.shape(proj_x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, I will use simple SVM for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(proj_x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize and project test data to eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002495697308915079 0.9161267079815967 (10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "norm_x_test = standardScaler.transform(x_test.reshape(-1,28*28))\n",
    "print(np.mean(norm_x_test), np.var(norm_x_test), np.shape(norm_x_test))\n",
    "proj_x_test = np.matmul(norm_x_test, eig_v)\n",
    "print(np.shape(proj_x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy : 0.9313166666666667\n",
      "test accuracy : 0.9265\n"
     ]
    }
   ],
   "source": [
    "train_correct = np.sum(clf.predict(proj_x_train) == y_train)\n",
    "test_correct = np.sum(clf.predict(proj_x_test) == y_test)\n",
    "train_accuracy = train_correct / np.shape(y_train)[0]\n",
    "test_accuracy = test_correct / np.shape(y_test)[0]\n",
    "print(\"train accuracy : \" + str(train_accuracy))\n",
    "print(\"test accuracy : \" + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quite good result :) Thank you for reading and Have a good day"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
