{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> Provided on April 23, Due on May 7 [BRI516, Spring/2020] </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For homework in general:\n",
    "* Install `Anaconda` and create an environment with `NumPy`, `Pandas`, `Matplotlib`, `scikit-learn` in Python 3.5\n",
    "* Please type the equation and/or text using markdown in jupyter-lab or jupyter-notebook\n",
    "* Please upload your jupyter-notebook file for homework to `Blackboard` (In case of 1.(a)-(c) and 2.(a)-(d), any format is fine; such as .docx, hand writing, etc.)\n",
    "* Please discuss your results at least one line of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Hw#2] \n",
    "\n",
    "##### (1) Linear discriminant analysis (LDA):\n",
    "\n",
    "Suppose we have two-classes and assume we have $m$-dimensional samples $\\{ \\bf{x}^1, \\bf{x}^2, \\cdots, \\bf{x}^{N_i} \\}$ belong to class $\\omega_i$, where $i \\in \\{1, 2\\}$.\n",
    "\n",
    "The aim is to obtain a transformation of $\\bf{x}$ to $y$ through projecting the samples in $\\bf{x}$ onto a line with a scalar $y$:\n",
    "$$ y = \\bf{w}^T \\bf{x} $$ \n",
    "where $\\bf{w}$ is a projection vector.\n",
    "\n",
    "(a) Show that an objective function to maximize for LDA can be represented as follows:\n",
    "\n",
    "$$ J(w) \\triangleq \\frac{|\\tilde{\\mu}_1 - \\tilde{\\mu}_2|^2}{\\tilde{s}_1^2 + \\tilde{s}_2^2} = \\frac{w^T S_B w}{w^T S_W w}, $$\n",
    "\n",
    "where $\\tilde{\\mu}_i$ and $\\tilde{s}_i^2$ are the mean value and variance of the $i^{th}$ class in the feature space $y$, respectively, and $\\bf{S}_W$ and $\\bf{S}_B$ are the within-class scatter matrix and between-class scatter matrix, respectively. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "(b) Show that the solution of the LDA can be given as the eigenvector of the following term:\n",
    "\n",
    "$$ \\bf{S}_X = \\bf{S}_W^{-1} \\bf{S}_B $$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "(c) Apply PCA and LDA to the MNIST digit dataset for feature extraction into two-dimensional space and compare the results.\n",
    "* Please use 'from sklearn import datasets' and 'load_digits()' to load MNIST dataset, then split them into train and test sets.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "(d) Apply the LR and SVM classifiers to the extracted features from (c) and compare the classification performance (i) between the two classifiers and (ii) between the original features and dimension reduced features. \n",
    "    \n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Kernel principal component analysis (KPCA):\n",
    "\n",
    "Suppose that the mean of the $d$-dimensional data in the kernal feature space is:\n",
    "$$ \\mu = \\frac{1}{n} \\sum^n_{i=1} \\phi (x_i) = 0 $$\n",
    "\n",
    "And, the covariance is :\n",
    "$$ C = \\frac{1}{n} \\sum^n_{i=1} \\phi ( x_i) {\\phi(x_i)}^T $$\n",
    "\n",
    "Thus, eigen-decomposition is as follows:\n",
    "$$ C \\bf{\\nu} = \\lambda \\bf{\\nu} $$\n",
    "\n",
    "(a) Show that the $j^{th}$ eigenvector can be expressed as a linear combination of features:\n",
    "\n",
    "$$ {\\bf{\\nu}}_j = \\sum^n_{i=1} \\alpha_{ji} \\phi(x_i), $$\n",
    "where $\\alpha_{ji}$ is a coefficient.\n",
    "\n",
    "<br>\n",
    "\n",
    "(b) Show that the coefficient $\\alpha_{ji}$ is obtained from the eigenvector of the kernel matrix:\n",
    "\n",
    "$$ K \\alpha_j = n\\lambda_j \\alpha_j, $$\n",
    "where $K_{ij} = K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j) $ \n",
    "\n",
    "<br>\n",
    "\n",
    "(c) Show that the zero-meaned kernel matrix is represented as follows:\n",
    "\n",
    "$$ \\tilde{K} = K - 2\\bf{1}_{1/n} K + \\bf{1}_{1/n} K \\bf{1}_{1/n}, $$\n",
    "where $\\bf{1}_{1/n}$ is a matrix with all elements $1/n$.\n",
    "\n",
    "<br>\n",
    "\n",
    "(d) Show that any data point, $x$ can be represented as:\n",
    "\n",
    "$$ y_j = \\sum^n_{i=1} \\alpha_{ji} K(x, x_i), j = 1, \\cdots, d $$\n",
    "\n",
    "<br>\n",
    "\n",
    "(e) Apply the KPCA to the MNIST digit data for two dimensional feature extraction and compare the results with (1-c).\n",
    "\n",
    "<br>\n",
    "\n",
    "(f) Apply the LR and SVM classifiers to the extracted features from (e) and compare the classification performance with (1-d)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
