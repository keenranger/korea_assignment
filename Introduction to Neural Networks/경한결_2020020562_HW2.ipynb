{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> Provided on April 23, Due on May 7 [BRI516, Spring/2020] </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For homework in general:\n",
    "* Install `Anaconda` and create an environment with `NumPy`, `Pandas`, `Matplotlib`, `scikit-learn` in Python 3.5\n",
    "* Please type the equation and/or text using markdown in jupyter-lab or jupyter-notebook\n",
    "* Please upload your jupyter-notebook file for homework to `Blackboard` (In case of 1.(a)-(c) and 2.(a)-(d), any format is fine; such as .docx, hand writing, etc.)\n",
    "* Please discuss your results at least one line of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Hw#2] \n",
    "\n",
    "##### (1) Linear discriminant analysis (LDA):\n",
    "\n",
    "Suppose we have two-classes and assume we have $m$-dimensional samples $\\{ \\bf{x}^1, \\bf{x}^2, \\cdots, \\bf{x}^{N_i} \\}$ belong to class $\\omega_i$, where $i \\in \\{1, 2\\}$.\n",
    "\n",
    "The aim is to obtain a transformation of $\\bf{x}$ to $y$ through projecting the samples in $\\bf{x}$ onto a line with a scalar $y$:\n",
    "$$ y = \\bf{w}^T \\bf{x} $$ \n",
    "where $\\bf{w}$ is a projection vector.\n",
    "\n",
    "(a) Show that an objective function to maximize for LDA can be represented as follows:\n",
    "\n",
    "$$ J(w) \\triangleq \\frac{|\\tilde{\\mu}_1 - \\tilde{\\mu}_2|^2}{\\tilde{s}_1^2 + \\tilde{s}_2^2} = \\frac{w^T S_B w}{w^T S_W w}, $$\n",
    "\n",
    "where $\\tilde{\\mu}_i$ and $\\tilde{s}_i^2$ are the mean value and variance of the $i^{th}$ class in the feature space $y$, respectively, and $\\bf{S}_W$ and $\\bf{S}_B$ are the within-class scatter matrix and between-class scatter matrix, respectively. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "(b) Show that the solution of the LDA can be given as the eigenvector of the following term:\n",
    "\n",
    "$$ \\bf{S}_X = \\bf{S}_W^{-1} \\bf{S}_B $$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "(c) Apply PCA and LDA to the MNIST digit dataset for feature extraction into two-dimensional space and compare the results.\n",
    "* Please use 'from sklearn import datasets' and 'load_digits()' to load MNIST dataset, then split them into train and test sets.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "(d) Apply the LR and SVM classifiers to the extracted features from (c) and compare the classification performance (i) between the two classifiers and (ii) between the original features and dimension reduced features. \n",
    "    \n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST digit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1797, 64)\n(1797,)\n"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "dataset = datasets.load_digits()\n",
    "x_data = dataset.data\n",
    "y_data = dataset.target\n",
    "print(np.shape(x_data))\n",
    "print(np.shape(y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split them into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1257, 64)\n(540, 64)\n(1257,)\n(540,)\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_data, y_data, test_size = 0.3, stratify = y_data)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_train))\n",
    "print(np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.11978151, 0.09575799])"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train_std, Y_train)\n",
    "X_test_lda = lda.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply LogisticRegression and SVM to PCA, LDA, original feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "lr_pca = LogisticRegression(solver='lbfgs', multi_class = 'auto')\n",
    "lr_pca = lr_pca.fit(X_train_pca, Y_train)\n",
    "svc_pca = svm.SVC()\n",
    "svc_pca = svc_pca.fit(X_train_pca, Y_train)\n",
    "lr_lda = LogisticRegression(solver='lbfgs', multi_class = 'auto')\n",
    "lr_lda = lr_lda.fit(X_train_lda, Y_train)\n",
    "svc_lda = svm.SVC()\n",
    "svc_lda = svc_lda.fit(X_train_lda, Y_train)\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class = 'auto')\n",
    "lr = lr.fit(X_train, Y_train)\n",
    "svc = svm.SVC()\n",
    "svc = svc.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.72      0.76      0.74        54\n           1       0.43      0.60      0.50        55\n           2       0.53      0.74      0.62        53\n           3       0.44      0.49      0.46        55\n           4       0.88      0.93      0.90        54\n           5       0.32      0.22      0.26        55\n           6       0.86      0.80      0.83        54\n           7       0.67      0.78      0.72        54\n           8       0.50      0.15      0.24        52\n           9       0.34      0.30      0.32        54\n\n    accuracy                           0.58       540\n   macro avg       0.57      0.58      0.56       540\nweighted avg       0.57      0.58      0.56       540\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98        54\n           1       0.45      0.53      0.49        55\n           2       0.44      0.42      0.43        53\n           3       0.55      0.58      0.57        55\n           4       0.96      0.91      0.93        54\n           5       0.46      0.33      0.38        55\n           6       0.88      0.94      0.91        54\n           7       0.75      0.81      0.78        54\n           8       0.43      0.40      0.42        52\n           9       0.48      0.52      0.50        54\n\n    accuracy                           0.64       540\n   macro avg       0.64      0.64      0.64       540\nweighted avg       0.64      0.64      0.64       540\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        54\n           1       0.95      0.98      0.96        55\n           2       0.98      0.96      0.97        53\n           3       0.96      0.95      0.95        55\n           4       1.00      0.94      0.97        54\n           5       0.96      1.00      0.98        55\n           6       1.00      0.98      0.99        54\n           7       0.96      1.00      0.98        54\n           8       0.88      0.88      0.88        52\n           9       0.98      0.98      0.98        54\n\n    accuracy                           0.97       540\n   macro avg       0.97      0.97      0.97       540\nweighted avg       0.97      0.97      0.97       540\n\n              precision    recall  f1-score   support\n\n           0       0.74      0.80      0.77        54\n           1       0.49      0.75      0.59        55\n           2       0.71      0.60      0.65        53\n           3       0.43      0.49      0.46        55\n           4       0.93      0.93      0.93        54\n           5       0.23      0.13      0.16        55\n           6       0.90      0.80      0.84        54\n           7       0.73      0.80      0.76        54\n           8       0.33      0.25      0.28        52\n           9       0.38      0.41      0.39        54\n\n    accuracy                           0.59       540\n   macro avg       0.58      0.59      0.58       540\nweighted avg       0.58      0.59      0.58       540\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98        54\n           1       0.47      0.62      0.54        55\n           2       0.50      0.28      0.36        53\n           3       0.53      0.64      0.58        55\n           4       0.98      0.91      0.94        54\n           5       0.40      0.42      0.41        55\n           6       0.91      0.96      0.94        54\n           7       0.78      0.74      0.76        54\n           8       0.42      0.48      0.45        52\n           9       0.48      0.39      0.43        54\n\n    accuracy                           0.64       540\n   macro avg       0.65      0.64      0.64       540\nweighted avg       0.65      0.64      0.64       540\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        54\n           1       0.95      1.00      0.97        55\n           2       1.00      1.00      1.00        53\n           3       1.00      0.98      0.99        55\n           4       1.00      0.98      0.99        54\n           5       0.98      1.00      0.99        55\n           6       1.00      1.00      1.00        54\n           7       0.98      1.00      0.99        54\n           8       1.00      0.94      0.97        52\n           9       0.98      0.98      0.98        54\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\n"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "print(classification_report(Y_test, lr_pca.predict(X_test_pca)))\n",
    "print(classification_report(Y_test, lr_lda.predict(X_test_lda)))\n",
    "print(classification_report(Y_test, lr.predict(X_test)))\n",
    "\n",
    "print(classification_report(Y_test, svc_pca.predict(X_test_pca)))\n",
    "print(classification_report(Y_test, svc_lda.predict(X_test_lda)))\n",
    "print(classification_report(Y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing PCA, LDA, and original feature, original featrue shows highest accuracy, and LDA follows. and PCA shows worst accuracy\n",
    "\n",
    "Comparing LR and SVC, SVC shows higher performance than LR among all features(PCA, LDA, original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Kernel principal component analysis (KPCA):\n",
    "\n",
    "Suppose that the mean of the $d$-dimensional data in the kernal feature space is:\n",
    "$$ \\mu = \\frac{1}{n} \\sum^n_{i=1} \\phi (x_i) = 0 $$\n",
    "\n",
    "And, the covariance is :\n",
    "$$ C = \\frac{1}{n} \\sum^n_{i=1} \\phi ( x_i) {\\phi(x_i)}^T $$\n",
    "\n",
    "Thus, eigen-decomposition is as follows:\n",
    "$$ C \\bf{\\nu} = \\lambda \\bf{\\nu} $$\n",
    "\n",
    "(a) Show that the $j^{th}$ eigenvector can be expressed as a linear combination of features:\n",
    "\n",
    "$$ {\\bf{\\nu}}_j = \\sum^n_{i=1} \\alpha_{ji} \\phi(x_i), $$\n",
    "where $\\alpha_{ji}$ is a coefficient.\n",
    "\n",
    "<br>\n",
    "\n",
    "(b) Show that the coefficient $\\alpha_{ji}$ is obtained from the eigenvector of the kernel matrix:\n",
    "\n",
    "$$ K \\alpha_j = n\\lambda_j \\alpha_j, $$\n",
    "where $K_{ij} = K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j) $ \n",
    "\n",
    "<br>\n",
    "\n",
    "(c) Show that the zero-meaned kernel matrix is represented as follows:\n",
    "\n",
    "$$ \\tilde{K} = K - 2\\bf{1}_{1/n} K + \\bf{1}_{1/n} K \\bf{1}_{1/n}, $$\n",
    "where $\\bf{1}_{1/n}$ is a matrix with all elements $1/n$.\n",
    "\n",
    "<br>\n",
    "\n",
    "(d) Show that any data point, $x$ can be represented as:\n",
    "\n",
    "$$ y_j = \\sum^n_{i=1} \\alpha_{ji} K(x, x_i), j = 1, \\cdots, d $$\n",
    "\n",
    "<br>\n",
    "\n",
    "(e) Apply the KPCA to the MNIST digit data for two dimensional feature extraction and compare the results with (1-c).\n",
    "\n",
    "<br>\n",
    "\n",
    "(f) Apply the LR and SVM classifiers to the extracted features from (e) and compare the classification performance with (1-d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply KPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=2)\n",
    "X_train_kpca = kpca.fit_transform(X_train_std)\n",
    "X_test_kpca = kpca.transform(X_test_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply LR and SVC to the feature extracted by KPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_kpca = LogisticRegression(solver='lbfgs', multi_class = 'auto')\n",
    "lr_kpca = lr_kpca.fit(X_train_kpca, Y_train)\n",
    "svc_kpca = svm.SVC()\n",
    "svc_kpca = svc_kpca.fit(X_train_kpca, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.72      0.76      0.74        54\n           1       0.43      0.60      0.50        55\n           2       0.53      0.74      0.62        53\n           3       0.44      0.49      0.46        55\n           4       0.88      0.93      0.90        54\n           5       0.32      0.22      0.26        55\n           6       0.86      0.80      0.83        54\n           7       0.67      0.78      0.72        54\n           8       0.50      0.15      0.24        52\n           9       0.34      0.30      0.32        54\n\n    accuracy                           0.58       540\n   macro avg       0.57      0.58      0.56       540\nweighted avg       0.57      0.58      0.56       540\n\n              precision    recall  f1-score   support\n\n           0       0.74      0.80      0.77        54\n           1       0.49      0.75      0.59        55\n           2       0.71      0.60      0.65        53\n           3       0.43      0.49      0.46        55\n           4       0.93      0.93      0.93        54\n           5       0.23      0.13      0.16        55\n           6       0.90      0.80      0.84        54\n           7       0.73      0.80      0.76        54\n           8       0.33      0.25      0.28        52\n           9       0.38      0.41      0.39        54\n\n    accuracy                           0.59       540\n   macro avg       0.58      0.59      0.58       540\nweighted avg       0.58      0.59      0.58       540\n\n"
    }
   ],
   "source": [
    "print(classification_report(Y_test, lr_kpca.predict(X_test_kpca)))\n",
    "print(classification_report(Y_test, svc_kpca.predict(X_test_kpca)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPCA shows quite similar result with PCA, or slightly higher accuracy.\n",
    "\n",
    "For extracted feature by KPCA, SVC shows higher accuracy than LR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('koreauniv': conda)",
   "language": "python",
   "name": "python37764bitkoreaunivconda41e9878bc8ff4c83b4feaefa96930e5d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}