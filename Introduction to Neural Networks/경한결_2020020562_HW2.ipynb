{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> Provided on April 23, Due on May 7 [BRI516, Spring/2020] </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For homework in general:\n",
    "* Install `Anaconda` and create an environment with `NumPy`, `Pandas`, `Matplotlib`, `scikit-learn` in Python 3.5\n",
    "* Please type the equation and/or text using markdown in jupyter-lab or jupyter-notebook\n",
    "* Please upload your jupyter-notebook file for homework to `Blackboard` (In case of 1.(a)-(c) and 2.(a)-(d), any format is fine; such as .docx, hand writing, etc.)\n",
    "* Please discuss your results at least one line of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None,  \n",
    "                          resolution=0.02):\n",
    "\n",
    "    # 마커와 컬러맵을 설정합니다\n",
    "    markers = ('s', 'x', 'o', '^', 'v','s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan','red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # 결정 경계를 그립니다\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=colors[idx],\n",
    "                    marker=markers[idx], label=cl, \n",
    "                    edgecolor='black')\n",
    "\n",
    "    # 테스트 샘플을 부각하여 그립니다7\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1],\n",
    "                    c='', edgecolor='black', alpha=1.0,\n",
    "                    linewidth=1, marker='o',\n",
    "                    s=100, label='test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Hw#2] \n",
    "\n",
    "##### (1) Linear discriminant analysis (LDA):\n",
    "\n",
    "Suppose we have two-classes and assume we have $m$-dimensional samples $\\{ \\bf{x}^1, \\bf{x}^2, \\cdots, \\bf{x}^{N_i} \\}$ belong to class $\\omega_i$, where $i \\in \\{1, 2\\}$.\n",
    "\n",
    "The aim is to obtain a transformation of $\\bf{x}$ to $y$ through projecting the samples in $\\bf{x}$ onto a line with a scalar $y$:\n",
    "$$ y = \\bf{w}^T \\bf{x} $$ \n",
    "where $\\bf{w}$ is a projection vector.\n",
    "\n",
    "(a) Show that an objective function to maximize for LDA can be represented as follows:\n",
    "\n",
    "$$ J(w) \\triangleq \\frac{|\\tilde{\\mu}_1 - \\tilde{\\mu}_2|^2}{\\tilde{s}_1^2 + \\tilde{s}_2^2} = \\frac{w^T S_B w}{w^T S_W w}, $$\n",
    "\n",
    "where $\\tilde{\\mu}_i$ and $\\tilde{s}_i^2$ are the mean value and variance of the $i^{th}$ class in the feature space $y$, respectively, and $\\bf{S}_W$ and $\\bf{S}_B$ are the within-class scatter matrix and between-class scatter matrix, respectively. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "(b) Show that the solution of the LDA can be given as the eigenvector of the following term:\n",
    "\n",
    "$$ \\bf{S}_X = \\bf{S}_W^{-1} \\bf{S}_B $$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "(c) Apply PCA and LDA to the MNIST digit dataset for feature extraction into two-dimensional space and compare the results.\n",
    "* Please use 'from sklearn import datasets' and 'load_digits()' to load MNIST dataset, then split them into train and test sets.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "(d) Apply the LR and SVM classifiers to the extracted features from (c) and compare the classification performance (i) between the two classifiers and (ii) between the original features and dimension reduced features. \n",
    "    \n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST digit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1797, 64)\n(1797,)\n"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "dataset = datasets.load_digits()\n",
    "x_data = dataset.data\n",
    "y_data = dataset.target\n",
    "print(np.shape(x_data))\n",
    "print(np.shape(y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split them into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1257, 64)\n(540, 64)\n(1257,)\n(540,)\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_data, y_data, test_size = 0.3, stratify = y_data)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_train))\n",
    "print(np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.12256986, 0.09837101])"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train_std, Y_train)\n",
    "X_test_lda = lda.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply LogisticRegression and SVM to PCA, LDA, original feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "lr_pca = LogisticRegression(solver='lbfgs', multi_class = 'auto')\n",
    "lr_pca = lr.fit(X_train_pca, Y_train)\n",
    "svc_pca = svm.SVC()\n",
    "svc_pca = svc.fit(X_train_pca, Y_train)\n",
    "lr_lda = LogisticRegression(solver='lbfgs', multi_class = 'auto')\n",
    "lr_lda = lr.fit(X_train_lda, Y_train)\n",
    "svc_lda = svm.SVC()\n",
    "svc_lda = svc.fit(X_train_lda, Y_train)\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class = 'auto')\n",
    "lr = lr.fit(X_train, Y_train)\n",
    "svc = svm.SVC()\n",
    "svc = svc.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        54\n           1       0.05      0.05      0.05        55\n           2       0.03      0.02      0.02        53\n           3       0.00      0.00      0.00        55\n           4       0.00      0.00      0.00        54\n           5       0.00      0.00      0.00        55\n           6       0.00      0.00      0.00        54\n           7       0.02      0.04      0.03        54\n           8       0.18      0.15      0.17        52\n           9       0.03      0.02      0.02        54\n\n    accuracy                           0.03       540\n   macro avg       0.03      0.03      0.03       540\nweighted avg       0.03      0.03      0.03       540\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96        54\n           1       0.75      0.89      0.82        55\n           2       0.84      0.77      0.80        53\n           3       0.73      0.75      0.74        55\n           4       0.93      0.80      0.86        54\n           5       0.67      0.73      0.70        55\n           6       0.73      0.85      0.79        54\n           7       0.62      0.59      0.60        54\n           8       0.59      0.56      0.57        52\n           9       0.43      0.37      0.40        54\n\n    accuracy                           0.73       540\n   macro avg       0.73      0.73      0.72       540\nweighted avg       0.73      0.73      0.72       540\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99        54\n           1       0.91      0.95      0.93        55\n           2       0.98      0.98      0.98        53\n           3       0.98      0.98      0.98        55\n           4       0.98      0.93      0.95        54\n           5       0.95      0.96      0.95        55\n           6       0.96      0.98      0.97        54\n           7       1.00      1.00      1.00        54\n           8       0.96      0.90      0.93        52\n           9       0.95      0.96      0.95        54\n\n    accuracy                           0.96       540\n   macro avg       0.97      0.96      0.96       540\nweighted avg       0.97      0.96      0.96       540\n\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        54\n           1       0.05      0.05      0.05        55\n           2       0.03      0.02      0.02        53\n           3       0.00      0.00      0.00        55\n           4       0.00      0.00      0.00        54\n           5       0.01      0.02      0.01        55\n           6       0.00      0.00      0.00        54\n           7       0.02      0.04      0.03        54\n           8       0.20      0.17      0.19        52\n           9       0.03      0.02      0.02        54\n\n    accuracy                           0.03       540\n   macro avg       0.04      0.03      0.03       540\nweighted avg       0.03      0.03      0.03       540\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.93      0.95        54\n           1       0.79      0.84      0.81        55\n           2       0.87      0.77      0.82        53\n           3       0.73      0.73      0.73        55\n           4       0.96      0.81      0.88        54\n           5       0.67      0.76      0.71        55\n           6       0.72      0.91      0.80        54\n           7       0.56      0.59      0.58        54\n           8       0.58      0.62      0.60        52\n           9       0.45      0.33      0.38        54\n\n    accuracy                           0.73       540\n   macro avg       0.73      0.73      0.73       540\nweighted avg       0.73      0.73      0.73       540\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        54\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        53\n           3       1.00      1.00      1.00        55\n           4       1.00      0.98      0.99        54\n           5       0.98      0.98      0.98        55\n           6       0.98      1.00      0.99        54\n           7       1.00      1.00      1.00        54\n           8       0.96      0.98      0.97        52\n           9       1.00      0.94      0.97        54\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\n"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "print(classification_report(Y_test, lr_pca.predict(X_test_pca)))\n",
    "print(classification_report(Y_test, lr_lda.predict(X_test_lda)))\n",
    "print(classification_report(Y_test, lr.predict(X_test)))\n",
    "\n",
    "print(classification_report(Y_test, svc_pca.predict(X_test_pca)))\n",
    "print(classification_report(Y_test, svc_lda.predict(X_test_lda)))\n",
    "print(classification_report(Y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing PCA, LDA, and original feature, original featrue shows highest accuracy, and LDA follows. and PCA shows worst accuracy\n",
    "\n",
    "Comparing LR and SVC, SVC shows higher performance than LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Kernel principal component analysis (KPCA):\n",
    "\n",
    "Suppose that the mean of the $d$-dimensional data in the kernal feature space is:\n",
    "$$ \\mu = \\frac{1}{n} \\sum^n_{i=1} \\phi (x_i) = 0 $$\n",
    "\n",
    "And, the covariance is :\n",
    "$$ C = \\frac{1}{n} \\sum^n_{i=1} \\phi ( x_i) {\\phi(x_i)}^T $$\n",
    "\n",
    "Thus, eigen-decomposition is as follows:\n",
    "$$ C \\bf{\\nu} = \\lambda \\bf{\\nu} $$\n",
    "\n",
    "(a) Show that the $j^{th}$ eigenvector can be expressed as a linear combination of features:\n",
    "\n",
    "$$ {\\bf{\\nu}}_j = \\sum^n_{i=1} \\alpha_{ji} \\phi(x_i), $$\n",
    "where $\\alpha_{ji}$ is a coefficient.\n",
    "\n",
    "<br>\n",
    "\n",
    "(b) Show that the coefficient $\\alpha_{ji}$ is obtained from the eigenvector of the kernel matrix:\n",
    "\n",
    "$$ K \\alpha_j = n\\lambda_j \\alpha_j, $$\n",
    "where $K_{ij} = K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j) $ \n",
    "\n",
    "<br>\n",
    "\n",
    "(c) Show that the zero-meaned kernel matrix is represented as follows:\n",
    "\n",
    "$$ \\tilde{K} = K - 2\\bf{1}_{1/n} K + \\bf{1}_{1/n} K \\bf{1}_{1/n}, $$\n",
    "where $\\bf{1}_{1/n}$ is a matrix with all elements $1/n$.\n",
    "\n",
    "<br>\n",
    "\n",
    "(d) Show that any data point, $x$ can be represented as:\n",
    "\n",
    "$$ y_j = \\sum^n_{i=1} \\alpha_{ji} K(x, x_i), j = 1, \\cdots, d $$\n",
    "\n",
    "<br>\n",
    "\n",
    "(e) Apply the KPCA to the MNIST digit data for two dimensional feature extraction and compare the results with (1-c).\n",
    "\n",
    "<br>\n",
    "\n",
    "(f) Apply the LR and SVM classifiers to the extracted features from (e) and compare the classification performance with (1-d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('koreauniv': conda)",
   "language": "python",
   "name": "python37764bitkoreaunivconda4dd430c95f3447b38c0c3676607cbdeb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}